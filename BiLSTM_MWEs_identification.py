# -*- coding: utf-8 -*-
"""BiLSTM_Rich_Classification_mwe_binaire_(2) (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F7ulP0aDufCE-dkiFkCcNV6foW6PAIkT

**Classification binaire avec l'oversamling**
"""

import pandas as pd
df_train = pd.read_csv('corpus_train.csv')
df_dev = pd.read_csv('corpus_dev.csv')
df_test = pd.read_csv('corpus_test.csv')

uniques = df_dev['mwe_tag'].unique()
uniques

uniques = df_test['mwe_tag'].unique()
uniques

uniques = df_train['mwe_tag'].unique()
uniques

# Utilisez .loc pour mettre à jour les lignes où 'mwe_tag' est égal à 'O'
df_train.loc[df_train['mwe_tag'] == 'O', 'mwe_tag'] = 'NonEP'
df_test.loc[df_test['mwe_tag'] == 'O', 'mwe_tag'] = 'NonEP'
df_dev.loc[df_dev['mwe_tag'] == 'O', 'mwe_tag'] = 'NonEP'

import nltk

print ("nbre total de mots : ",len (df_train.	word_form))
word_dist = nltk.FreqDist(df_train.	word_form)
print ("vocabulaire :", len (word_dist))

df_train.drop(['sentence_index', 'morph_tag', 'token_index', 'dep_relation', 'Unnamed: 0'], axis = 1, inplace = True)
df_dev.drop(['sentence_index', 'morph_tag', 'token_index', 'dep_relation', 'Unnamed: 0'], axis = 1, inplace = True)
df_test.drop(['sentence_index', 'morph_tag', 'token_index', 'dep_relation', 'Unnamed: 0'], axis = 1, inplace = True)

df_train

for i in range(len(df_train)):
  if df_train['mwe_tag'].iloc[i] in ['LVC.full','VID', 'IAV', 'LVC.cause']:
    df_train['mwe_tag'].iloc[i] = 'EP'
  else:
    df_train['mwe_tag'].iloc[i] = 'NonEP'

for i in range(len(df_dev)):
  if df_dev['mwe_tag'].iloc[i] in ['LVC.full','VID', 'IAV', 'LVC.cause']:
    df_dev['mwe_tag'].iloc[i] = 'EP'
  else:
    df_dev['mwe_tag'].iloc[i] = 'NonEP'

for i in range(len(df_test)):
  if df_test['mwe_tag'].iloc[i] in ['LVC.full','VID', 'IAV', 'LVC.cause']:
    df_test['mwe_tag'].iloc[i] = 'EP'
  else:
    df_test['mwe_tag'].iloc[i] = 'NonEP'

# Balance of train data
import matplotlib.pyplot as plt
df_train.groupby('mwe_tag').count().plot(kind='bar')
plt.show()

# Train set value counts
df_train.groupby('mwe_tag').count()

# Randomly selecting 700 indices in classes with low value count
import numpy as np
to_add = np.random.choice(df_train[df_train['mwe_tag']=='EP'].index,size = 9798,replace=False)


len(to_add)

# Forming a dataframe for randomly selected indices
df_replicate = df_train[df_train.index.isin(to_add)]
df_replicate

# Concatenating replicated df to orinigal df
train_data = pd.concat([df_train, df_replicate])
train_data['mwe_tag'].value_counts()

train_data.shape

train_word = train_data['word_form']
train_lemma = train_data['lemma_form']
dev_word = df_dev['word_form']
dev_lemma = df_dev['lemma_form']
test_word = df_test['word_form']
test_lemma = df_test['lemma_form']

from tensorflow.keras.preprocessing.text import Tokenizer

# Defining training parameters

max_words = 241121
train_word = train_word.to_frame()
dev_word = dev_word.to_frame()
test_word = test_word.to_frame()
# Tokenizing 	word_forms/sentences wrt num_words
tokenizer = Tokenizer(num_words = max_words)  # Selects most frequent words
tokenizer.fit_on_texts(train_word.	word_form)      # Develops internal vocab based on training text
word_train_sequences = tokenizer.texts_to_sequences(train_word.	word_form)  # converts text to sequence
word_dev_sequences = tokenizer.texts_to_sequences(dev_word.	word_form)
word_test_sequences = tokenizer.texts_to_sequences(test_word.	word_form)

# Assume that word_train_sequences is a list of tokenized sequences

# Trouver la longueur maximale des séquences
max_sequence_length = max(len(sequence) for sequence in word_train_sequences)

print("Longueur maximale des séquences :", max_sequence_length)

# Fixing the sequence length
max_sequence_length = 1
from tensorflow.keras.preprocessing.sequence import pad_sequences
train_word = pad_sequences(word_train_sequences, maxlen = max_sequence_length)
dev_word = pad_sequences(word_dev_sequences, maxlen = max_sequence_length)
test_word = pad_sequences(word_test_sequences, maxlen = max_sequence_length)
train_word.shape, dev_word.shape, test_word.shape

# Importing Libraries
from tensorflow.keras.preprocessing.text import Tokenizer
import tensorflow as tf
import sys, os, re, csv, codecs, numpy as np, pandas as pd
from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers
from tensorflow.keras.layers import Dense, Input, Input, Flatten, Dropout, BatchNormalization
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding

import gensim
model_word2vec = gensim.models.Word2Vec.load('full_grams_cbow_300_wiki.mdl')

word_vectors = model_word2vec.wv

import numpy as np
embedding_dim = 300
max_words = 241121
embedding_word = np.zeros((max_words, embedding_dim))

for word, i in tokenizer.word_index.items():
    if i < max_words:
        if word in word_vectors:
            embedding_word[i] = word_vectors[word]

train_lemma = train_lemma.to_frame()
dev_lemma = dev_lemma.to_frame()
test_lemma = test_lemma.to_frame()
# Tokenizing 	word_forms/sentences wrt num_words
tokenizer = Tokenizer(num_words = max_words)  # Selects most frequent words
tokenizer.fit_on_texts(train_lemma.	lemma_form)      # Develops internal vocab based on training text
lemma_train_sequences = tokenizer.texts_to_sequences(train_lemma.	lemma_form)  # converts text to sequence
lemma_dev_sequences = tokenizer.texts_to_sequences(dev_lemma.	lemma_form)
lemma_test_sequences = tokenizer.texts_to_sequences(test_lemma.	lemma_form)

# Fixing the sequence length
from tensorflow.keras.preprocessing.sequence import pad_sequences
train_lemma = pad_sequences(lemma_train_sequences, maxlen = max_sequence_length)
dev_lemma = pad_sequences(lemma_dev_sequences, maxlen = max_sequence_length)
test_lemma = pad_sequences(lemma_test_sequences, maxlen = max_sequence_length)
train_lemma.shape, dev_lemma.shape, test_lemma.shape

model_lemm2vec = gensim.models.Word2Vec.load('full_grams_cbow_300_wiki.mdl')
lemm_vectors = model_lemm2vec.wv

import numpy as np
embedding_dim = 300
max_lemms =  241121
embedding_lemm = np.zeros((max_lemms, embedding_dim))

for lemm, i in tokenizer.word_index.items():
    if i < max_lemms:
        if lemm in lemm_vectors:
            embedding_lemm[i] = lemm_vectors[lemm]

# Importer les modules nécessaires
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Conv1D, MaxPooling1D, GlobalMaxPool1D
from tensorflow.keras.initializers import glorot_uniform  # Importez l'initialisation de Xavier


# Créer les couches d'entrée pour le mot et le lemme
word_input = Input(shape=(max_sequence_length,), dtype=tf.int32, name="word_input")
lemma_input = Input(shape=(max_sequence_length,), dtype=tf.int32, name="lemma_input")

# Créer les couches d'embedding pour le mot et le lemme avec initialisation de Xavier (glorot_uniform)
word_embed = Embedding(max_words, embedding_dim, weights=[embedding_word], trainable=False,
                    embeddings_initializer=glorot_uniform(), name="word_embed")(word_input)
lemma_embed = Embedding(max_words, embedding_dim, weights=[embedding_lemm], trainable=False,
                    embeddings_initializer=glorot_uniform(), name="lemma_embed")(lemma_input)

# Concaténer les embeddings des mots et des lemmes
concat_embed = tf.concat([word_embed, lemma_embed], axis=-1)

# Couche Conv1D pour extraire des caractéristiques pertinentes avec initialisation de Xavier (glorot_uniform)
conv1d = Conv1D(filters=64, kernel_size=1, padding='valid', activation='relu', kernel_initializer=glorot_uniform(), name="conv1d")(concat_embed)

# Couche MaxPooling pour réduire la dimensionnalité
maxpooling1d = MaxPooling1D(pool_size=1, name="maxpooling1d")(conv1d)



# Couche LSTM bidirectionnelle 1 avec dropout et recurrent dropout
bilstm1 = Bidirectional(LSTM(64, return_sequences=True), name="bilstm1")(maxpooling1d)
bilstm2 = Bidirectional(LSTM(64, return_sequences=True), name="bilstm2")(bilstm1)

# Couche de pooling globale
globalmaxpooling1d = GlobalMaxPool1D(name="globalmaxpooling1d")(bilstm1)

# Couche de sortie avec activation softmax
output = Dense(2, activation='sigmoid', name="output")(globalmaxpooling1d)



# Créer le modèle
model = Model(inputs=[word_input, lemma_input], outputs=output)

# Afficher le résumé du modèle
model.summary()

from sklearn.preprocessing import LabelEncoder
from keras.utils import to_categorical

# Déclarer les étiquettes
train_labels = train_data['mwe_tag']
dev_labels = df_dev['mwe_tag']
test_labels = df_test['mwe_tag']

# Encoder les étiquettes avec LabelEncoder
le = LabelEncoder()
le.fit(train_labels)
train_labels_binary = le.transform(train_labels)
dev_labels_binary = le.transform(dev_labels)
test_labels_binary = le.transform(test_labels)

# Afficher les classes de LabelEncoder
print(le.classes_)

# Afficher les statistiques des étiquettes encodées
print(np.unique(train_labels_binary, return_counts=True))
print(np.unique(dev_labels_binary, return_counts=True))
print(np.unique(test_labels_binary, return_counts=True))

# Convertir les étiquettes encodées en catégorielles (one-hot encoding) pour une classification binaire
num_classes = 2  # Car c'est une classification binaire
train_labels_categorical = to_categorical(train_labels_binary, num_classes=num_classes)
dev_labels_categorical = to_categorical(dev_labels_binary, num_classes=num_classes)
test_labels_categorical = to_categorical(test_labels_binary, num_classes=num_classes)

# Compilez le modèle
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])


# Entraîner le modèle avec des données d'entraînement et de validation
history = model.fit(x=[train_word, train_lemma],
          y=train_labels_categorical,
          epochs=100,
          batch_size=128,
          validation_data=([dev_word, dev_lemma], dev_labels_categorical))

# Evaluate model on Development dev
from sklearn.metrics import classification_report
from sklearn.metrics import f1_score


y_pred = model.predict([dev_word, dev_lemma])
y_pred_classes = (y_pred > 0.5).astype(int)

print(classification_report(dev_labels_categorical, y_pred_classes))

# Evaluate model on Test Set
y_pred_test = model.predict([test_word, test_lemma])
y_pred_classes = (y_pred_test > 0.5).astype(int)

print(classification_report(test_labels_categorical, y_pred_classes))







