# -*- coding: utf-8 -*-
"""BiLSTM_Rich_Classification_mwe_multi (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qA_r1QGoE9fuLI19vpPBCjRnFlUJ4MbU
"""

import pandas as pd
df_train = pd.read_csv('/content/drive/MyDrive/corpus_train.csv')
df_dev = pd.read_csv('/content/drive/MyDrive/corpus_dev.csv')
df_test = pd.read_csv('/content/drive/MyDrive/corpus_test.csv')

uniques = df_train['mwe_tag'].unique()
uniques

import nltk

print ("nbre total de mots : ",len (df_train.	word_form))
word_dist = nltk.FreqDist(df_train.	word_form)
print ("vocabulaire :", len (word_dist))

df_train.drop(['sentence_index', 'morph_tag', 'token_index', 'dep_relation', 'Unnamed: 0'], axis = 1, inplace = True)
df_dev.drop(['sentence_index', 'morph_tag', 'token_index', 'dep_relation', 'Unnamed: 0'], axis = 1, inplace = True)
df_test.drop(['sentence_index', 'morph_tag', 'token_index', 'dep_relation', 'Unnamed: 0'], axis = 1, inplace = True)

# Utilisez .loc pour mettre à jour les lignes où 'mwe_tag' est égal à 'O'
df_train.loc[df_train['mwe_tag'] == 'O', 'mwe_tag'] = 'NonEP'
df_dev.loc[df_dev['mwe_tag'] == 'O', 'mwe_tag'] = 'NonEP'
df_test.loc[df_test['mwe_tag'] == 'O', 'mwe_tag'] = 'NonEP'

df_train

# Balance of train data
import matplotlib.pyplot as plt
df_train.groupby('mwe_tag').count().plot(kind='bar')
plt.show()

# Train set value counts
df_train.groupby('mwe_tag').count()

# Randomly selecting 700 indices in classes with low value count
import numpy as np
to_add_1 = np.random.choice(df_train[df_train['mwe_tag']=='IAV'].index,size = 1192,replace=False)
to_add_2 = np.random.choice(df_train[df_train['mwe_tag']=='LVC.cause'].index,size = 695,replace=False)
to_add_3 = np.random.choice(df_train[df_train['mwe_tag']=='LVC.full'].index,size = 5830,replace=False)
to_add_4 = np.random.choice(df_train[df_train['mwe_tag']=='VID'].index,size=2081,replace=False)

# Indices to be added
to_add = np.concatenate((to_add_1, to_add_2, to_add_3, to_add_4 ))
len(to_add)

# Forming a dataframe for randomly selected indices
df_replicate = df_train[df_train.index.isin(to_add)]
df_replicate

# Concatenating replicated df to orinigal df
df_train = pd.concat([df_train, df_replicate])
df_train['mwe_tag'].value_counts()

# Declaring train labels
train_labels = df_train['mwe_tag']
dev_labels = df_dev['mwe_tag']
test_labels = df_test['mwe_tag']

train_word = df_train['word_form']
train_lemma = df_train['lemma_form']
dev_word = df_dev['word_form']
dev_lemma = df_dev['lemma_form']
test_word = df_test['word_form']
test_lemma = df_test['lemma_form']

# Converting labels to numerical features
import numpy as np
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
le.fit(train_labels)
train_labels = le.transform(train_labels)
dev_labels = le.transform(dev_labels)
test_labels = le.transform(test_labels)

print(le.classes_)
print(np.unique(train_labels, return_counts=True))
print(np.unique(test_labels, return_counts=True))

# Changing labels to categorical features
import numpy as np
from tensorflow.python.keras.utils import np_utils
from tensorflow.keras.utils import to_categorical
import numpy as np

train_labels = to_categorical(np.asarray(train_labels))
dev_labels = to_categorical(np.array(dev_labels))
test_labels = to_categorical(np.array(test_labels))

from tensorflow.keras.preprocessing.text import Tokenizer

# Defining training parameters
max_sequence_length = 1
max_words = 129237
train_word = train_word.to_frame()
dev_word = dev_word.to_frame()
test_word = test_word.to_frame()
# Tokenizing 	word_forms/sentences wrt num_words
tokenizer = Tokenizer(num_words = max_words)  # Selects most frequent words
tokenizer.fit_on_texts(train_word.	word_form)      # Develops internal vocab based on training text
word_train_sequences = tokenizer.texts_to_sequences(train_word.	word_form)  # converts text to sequence
word_dev_sequences = tokenizer.texts_to_sequences(dev_word.	word_form)
word_test_sequences = tokenizer.texts_to_sequences(test_word.	word_form)

# Fixing the sequence length
from tensorflow.keras.preprocessing.sequence import pad_sequences
train_word = pad_sequences(word_train_sequences, maxlen = max_sequence_length)
dev_word = pad_sequences(word_dev_sequences, maxlen = max_sequence_length)
test_word = pad_sequences(word_test_sequences, maxlen = max_sequence_length)
train_word.shape, dev_word.shape, test_word.shape



# Importing Libraries
from tensorflow.keras.preprocessing.text import Tokenizer
import tensorflow as tf
import sys, os, re, csv, codecs, numpy as np, pandas as pd
from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, Conv1D, SimpleRNN
from tensorflow.keras.models import Model
from tensorflow.keras.models import Sequential
from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers
from tensorflow.keras.layers import Dense, Input, Input, Flatten, Dropout, BatchNormalization
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Embedding

###################################Word2Vec+Oversampling

import gensim
from gensim.models import Word2Vec
model_word2vec = gensim.models.Word2Vec.load('/content/drive/MyDrive/full_grams_cbow_300_wiki.mdl')

word_vectors = model_word2vec.wv

import numpy as np
embedding_dim = 300
max_words = 241121
embedding_word = np.zeros((max_words, embedding_dim))

for word, i in tokenizer.word_index.items():
    if i < max_words:
        if word in word_vectors:
            embedding_word[i] = word_vectors[word]

train_lemma = train_lemma.to_frame()
dev_lemma = dev_lemma.to_frame()
test_lemma = test_lemma.to_frame()
# Tokenizing 	word_forms/sentences wrt num_words
tokenizer = Tokenizer(num_words = max_words)  # Selects most frequent words
tokenizer.fit_on_texts(train_lemma.	lemma_form)      # Develops internal vocab based on training text
lemma_train_sequences = tokenizer.texts_to_sequences(train_lemma.	lemma_form)  # converts text to sequence
lemma_dev_sequences = tokenizer.texts_to_sequences(dev_lemma.	lemma_form)
lemma_test_sequences = tokenizer.texts_to_sequences(test_lemma.	lemma_form)

# Fixing the sequence length
from tensorflow.keras.preprocessing.sequence import pad_sequences
train_lemma = pad_sequences(lemma_train_sequences, maxlen = max_sequence_length)
dev_lemma = pad_sequences(lemma_dev_sequences, maxlen = max_sequence_length)
test_lemma = pad_sequences(lemma_test_sequences, maxlen = max_sequence_length)
train_lemma.shape, dev_lemma.shape, test_lemma.shape



from google.colab import drive
drive.mount('/content/drive')

model_lemm2vec = gensim.models.Word2Vec.load('/content/drive/MyDrive/full_grams_cbow_300_wiki.mdl')
lemm_vectors = model_lemm2vec.wv

import numpy as np
embedding_dim = 300
max_lemms = 241121
embedding_lemm = np.zeros((max_lemms, embedding_dim))

for lemm, i in tokenizer.word_index.items():
    if i < max_lemms:
        if lemm in lemm_vectors:
            embedding_lemm[i] = lemm_vectors[lemm]

# Importer les modules nécessaires
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, LSTM, Bidirectional, Dense, Conv1D, MaxPooling1D, GlobalMaxPool1D
from tensorflow.keras.initializers import glorot_uniform  # Importez l'initialisation de Xavier

num_classes = 5 # Nombre de classes à prédire

# Créer les couches d'entrée pour le mot et le lemme
word_input = Input(shape=(max_sequence_length,), dtype=tf.int32, name="word_input")
lemma_input = Input(shape=(max_sequence_length,), dtype=tf.int32, name="lemma_input")

# Créer les couches d'embedding pour le mot et le lemme avec initialisation de Xavier (glorot_uniform)
word_embed = Embedding(max_words, embedding_dim, weights=[embedding_word], trainable=False,
                    embeddings_initializer=glorot_uniform(), name="word_embed")(word_input)
lemma_embed = Embedding(max_words, embedding_dim, weights=[embedding_lemm], trainable=False,
                    embeddings_initializer=glorot_uniform(), name="lemma_embed")(lemma_input)

# Concaténer les embeddings des mots et des lemmes
concat_embed = tf.concat([word_embed, lemma_embed], axis=-1)

# Couche Conv1D pour extraire des caractéristiques pertinentes avec initialisation de Xavier (glorot_uniform)
conv1d = Conv1D(filters=64, kernel_size=1, padding='valid', activation='relu', kernel_initializer=glorot_uniform(), name="conv1d")(concat_embed)

# Couche MaxPooling pour réduire la dimensionnalité
maxpooling1d = MaxPooling1D(pool_size=1, name="maxpooling1d")(conv1d)

# Couche LSTM bidirectionnelle 1 avec dropout et recurrent dropout
bilstm = Bidirectional(LSTM(64, return_sequences=True), name="bilstm")(maxpooling1d)

# Couche de pooling globale
globalmaxpooling1d = GlobalMaxPool1D(name="globalmaxpooling1d")(bilstm)

# Couche de sortie avec activation sigmoid
output = Dense(num_classes, activation='softmax', name="output")(globalmaxpooling1d)







# Créer le modèle
model = Model(inputs=[word_input, lemma_input], outputs=output)

# Afficher le résumé du modèle
model.summary()

# Compilez le modèle
model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])

# Entraînez le modèle en dehors de la fonction model.fit
history = model.fit([train_word, train_lemma], train_labels, epochs=200, batch_size=128, validation_data=([dev_word, dev_lemma], dev_labels))

# Après l'entraînement, vous pouvez analyser l'historique (loss, accuracy, etc.)

# Prediction on dev Data
predicted_bi_lstm = model.predict([dev_word, dev_lemma])
predicted_bi_lstm

import sklearn
from sklearn.metrics import precision_recall_fscore_support as score
precision, recall, fscore, support = score(dev_labels, predicted_bi_lstm.round())

print('precision: {}'.format(precision))
print('recall: {}'.format(recall))
print('fscore: {}'.format(fscore))
print('support: {}'.format(support))
print('################################')
print(sklearn.metrics.classification_report(dev_labels, predicted_bi_lstm.round()))

# Prediction on test Data
predicted_bi_lstm = model.predict([test_word, test_lemma])
predicted_bi_lstm

import sklearn
from sklearn.metrics import precision_recall_fscore_support as score
precision, recall, fscore, support = score(test_labels, predicted_bi_lstm.round())

print('precision: {}'.format(precision))
print('recall: {}'.format(recall))
print('fscore: {}'.format(fscore))
print('support: {}'.format(support))
print('################################')
print(sklearn.metrics.classification_report(test_labels, predicted_bi_lstm.round()))



def accuracy_plot(history):

    fig, ax = plt.subplots(1, 2, figsize=(12,5))

    fig.suptitle('Model Performance with Epochs', fontsize = 16)
    # Subplot 1
    ax[0].plot(history.history['accuracy'])
    ax[0].plot(history.history['val_accuracy'])
    ax[0].set_title('Model Accuracy', fontsize = 14)
    ax[0].set_xlabel('Epochs', fontsize = 12)
    ax[0].set_ylabel('Accuracy', fontsize = 12)
    ax[0].legend(['train', 'validation'], loc='best')

    # Subplot 2
    ax[1].plot(history.history['loss'])
    ax[1].plot(history.history['val_loss'])
    ax[1].set_title('Model Loss', fontsize = 14)
    ax[1].set_xlabel('Epochs', fontsize = 12)
    ax[1].set_ylabel('Loss', fontsize = 12)
    ax[1].legend(['train', 'validation'], loc='best')


accuracy_plot(history)

# Declaring function for plotting confusion matrix
import seaborn as sns
from sklearn.metrics import confusion_matrix

def plot_cm(model, test_data, test_labels):

    mwe = ['IAV', 'LVC.cause', 'LVC.full', 'NonVMWE', 'VID']

    # Declaring confusion matrix
    cm = confusion_matrix(np.argmax(np.array(test_labels),axis=1), np.argmax(predicted_bi_lstm, axis=1))

    # Heat map labels

    group_counts = ['{0:0.0f}'.format(value) for value in cm.flatten()]
    group_percentages = ['{0:.2%}'.format(value) for value in cm.flatten()/np.sum(cm)]

    labels = [f"{v2}\n{v3}" for v2, v3 in zip(group_counts, group_percentages)]
    labels = np.asarray(labels).reshape(5,5)

    # Plotting confusion matrix
    plt.figure(figsize=(12,8))

    sns.heatmap(cm, cmap=plt.cm.Blues, annot=labels, annot_kws={"size": 15}, fmt = '',
                xticklabels = mwe,
                yticklabels = mwe)

    plt.xticks(fontsize = 12)
    plt.yticks(fontsize = 12, rotation = 'horizontal')
    plt.title('Confusion Matrix\n', fontsize=19)
    plt.xlabel('Predicted Labels', fontsize=17)
    plt.ylabel('Actual Labels', fontsize=17)

plot_cm(model, test_data, test_labels)







